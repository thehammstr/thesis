% !TEX root = ../Thesis.tex

\chapter{Loop Closure Detection}
\label{ch.LoopClosure}

One of the most challenging problems with mapping icebergs is automatic loop closure detection. Solving the correspondence problem is crucial to estimating iceberg motion, but difficult precisely because of that motion. This chapter looks more closely at how this difficulty arises, and proposes two methods for solving the correspondence problem for underwater mapping tasks where precise inertial navigation is unavailable. 

The first method is a streamlined graphical interface that allows a human to direct the search for loop closure events. An automated algorithm then operates on a reduced search space to perform precise alignment, and the user can decide whether the alignment is precise enough to include in the mapping optimization.

The second method comprises early work toward full automation of the loop closure detection process. It borrows heavily from computer vision and image processing literature, applying robust image feature extraction techniques to sonar range data in order to detect loop closure. 

\section{The Challenge of Loop Closure Detection}

Inertial navigation errors, whether introduced by moving terrain or poor sensing, complicates the task of loop closure detection in several ways. This section gives a brief review of the underwater mapping technique described in Chapter \ref{ch.RelatedWork} to emphasize the role of loop closure in navigation and mapping. It then describes the particular challenges that arise from these errors.

\subsection{Underwater mapping revisited}

Accurate robotic mapping requires accurate localization within the environment being mapped, as described in Chapter \ref{ch.Introduction}. Underwater robots use a combination of infrastructure, inertial navigation, and environment-aided navigation to localize themselves. 

Nearly all mapping missions rely on information in the terrain to refine the localization and produce a self-consistent map. This process was described in Chapter \ref{ch.RelatedWork}, but for convenience, it is again stated here.

The AUVs are pre-programmed to fly self-intersecting trajectories, and have accurate enough inertial guidance as to guarantee sufficient overlap. Thus the correspondence problem is solved automatically. Submaps are extracted and correlated to estimate drift, and the trajectory is altered in such a way as to distribute the error as unobtrusively as possible, while driving the measured drift to zero.

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=.7\textwidth]{../graphics/NavPerformance.jpg} % requires the graphicx package
   \caption{Illustration of normal bathymetric mapping procedure. \emph{www.mbari.org} }
   \label{fig:BathyMapping2}
\end{figure}


% topic paragraph: summarizes the rest, but does it make sense before what is to follow?

\subsection{Complications due to motion}

The iceberg motion introduces larger odometry errors, which complicates the loop closure detection process in two ways: first, a larger area much be searched in order to guarantee overlap, and second, the submaps themselves can become warped relative to the true terrain shape.

%All inertial navigation systems drift over time without bound. This is a fundamental limitation of obtaining a position estimate solely by integrating noisy velocity and acceleration measurements. A modern state-of-the art INS can provide drift rates as low as 0.01\% of the distance traveled, but even this low a drift rate will eventually grow large enough to introduce substantial map error.  

Submaps are built at strategic locations where the planned trajectory self-intersects. The maps must be large enough and the crossings frequent enough that overlap is guaranteed, based on the inertial drift characeristics of the vehicle. Less accurate odometry or longer intervals between crossings will cause larger potential drift, and require a larger submap to ensure overlap. 

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=.7\textwidth]{../graphics/GraphSLAMcartoons/deadReckoning.png} % requires the graphicx package
   \caption{The black points are the estimated/planned trajectory. The red points are the actual trajectory. Blurred ellipses show the positional uncertainty due to dead reckoning. The greater the inertial navigation error, the larger the search ellipse for loop closure detection.  }
   \label{fig:BathyMapping2}
\end{figure}


The second complication arises from the implicit assumption of submap rigidity. The submap method assumes that the inertial guidance of the vehicle is extremely accurate over the short time it takes to build a submap. Any navigation errors during that time will introduce warping to the submap, which can cause the registration method to fail or worse: create a positive match on different terrain. 

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/warpage.png} % requires the graphicx package
   \caption{The effect of heading drift rate error on map warping. In red is a high elevation view of 11 minutes of data from a vertical section of Soquel Canyon. It is projected with no heading rate error. The green and blue clouds are the same data, but have heading rate errors of $\pm0.001$ rad/s. Note how the edges of the warped map curl away from truth.}
   \label{fig:motionErrorWarping}
\end{figure}

The first issue drives a requirement for increased submap size. However, a larger submap means that more scans have to be concatenated over a longer time. This gives the motion-induced odometry error more chance to warp the submap. This drives the requirement to use as small a submap as possible. Somehow, a balance must be struck between these competing needs. 

%One possible solution to achieve large coverage area with minimal map warping might be to search for the best registration over multiple smaller submaps, and devise a metric to select the best submap alignment pair. However, as odometry error increases, the search space can become large. Point cloud registrations are relatively expensive, and verifying their alignment can be difficult. 

These concerns provide the motivation for the methods developed in the rest of this chapter. The first one, a graphical user interface, leverages humans' excellent pattern recognition ability to provide high level guidance and error checking for the alignment algorithm. It provides a streamlined method for encoding loop closure information for missions where human intervention is feasible. 

The second method continues working toward the goal of removing the need for human intervention entirely. Borrowing from well-developed concepts in the computer vision and image processing communities, and more recent work in point cloud feature extraction, it seeks to find local, stable, identifiable features in the terrain that can be recognized upon a second observation and used to initialize more accurate point cloud registration methods. 

\section{Loop Closure GUI}
\label{sec:GUI}

This section describes in detail the graphical user interface designed to allow a user to identify and encode loop closures quickly and easily. 

\subsection{GUI Operation}

Loop closure GUI is meant to be straightforward. As a general rule, the more loop closures that are incorporated into the optimization, the more accurately the iceberg motion can be accounted for, yielding a more accurate and self-consistent map. The user launches the program including an argument that points to the log file including the navigation and sonar data. The vehicle path and multibeam cloud are rendered in the main window. The user can then look for areas of likely overlap and encode links incrementally.

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/GUI.png} % requires the graphicx package
   \caption{Loop closure GUI being used to encode loop closures on Soquel Canyon data set }
   \label{fig:GUI}
\end{figure}

The user drags the slider bars at the top left, labeled ``first index" and ``second index," respectively. These indices correspond to the reference time stamp of the vehicle trajectory data. In the main data window, a white line is rendered that connects these two indices, as feedback for the user. 

A submap is then automatically constructed around these two indices. To do this, the sonar measurements for the $N$ preceding scans and $N$ scans to follow are concatenated into one rigid submap, based on the vehicle's estimated trajectory over that time. Thus each submap contains $2N+1$ multibeam scans, associated with $2N+1$ consecutive vehicle poses. $N$ can be changed by the user using the ``Submap width" slider. Each point cloud is transformed into its central pose's reference frame, with the central pose defined as the origin. Because the vehicle attempts to maintain a constant standoff distance from the icberg surface, this tends to provide a reasonable initialization for the point cloud registration techniques that will later be applied. An implicit assumption is that the motion of the iceberg or the inertial drift of the vehicle during such a short time is small enough that it can safely be neglected. The two submaps are displayed in the lower window of the GUI. Additionally, the extent of the two submaps is overlaid on the full data set in the top window to aid the user in selecting areas of likely overlap.

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=.9\textwidth]{../graphics/GUIcloseup.png} % requires the graphicx package
   \caption{Closeup of Figure \ref{fig:GUI}. A white line between the two candidate indices is rendered as visual feedback for the user Also, the extent of the submaps used for registration is overlaid on the raw data in red and yellow. The green lines are loop closure events that have already been recorded.}
   \label{fig:GUIcloseup}
\end{figure}

Once the user determines that the two submaps likely correspond to the same terrain, he or she can click the ``ICP" button. This applies the iterative closest point algorithm to the two submaps and displays the result to the lower window. For the iceberg implementation, the depth, pitch, and roll of the vehicle is assumed to be known, allowing a reduced model to be used for point cloud registration. Only the horizontal offsets $dx$ and $dy$, and heading offset $d\psi$ are estimated by the algorithm. Extending this method to the full six degree-of-freedom case would require the full estimation model, but using the reduced model makes the algorithm more robust to bad initial alignments, and is sufficient for this application. 
 
 \begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/misaligned_subcloud.png} % requires the graphicx package
   \caption{The bottom window of the GUI shows the two subclouds in detail. The window is fully interactive, so the user can view the clouds from any angle, zoom, etc. Before ICP, these two clouds display some misalignment. The right image shows the top view, to highlight the misalignment.}
   \label{fig:GUI_preicp}
\end{figure}

The user can then inspect the resulting point clouds to determine whether ICP converged to a good alignment. If it has, the user clicks the ``Accept/write" button, and the position and heading offset between the two reference indices are recorded to file immediately. Otherwise, the user can choose different map points, submap widths, etc. and try again. If a bad alignment is accidentally recorded, the ``Delete Last" button removes it. Upon acceptance, the white line connecting the two reference indices turns green, and persists in the main display until it is either deleted or the session ends. This helps the user remember which areas already have loop closure information, and which still need to be processed. These lines can be seen in Figures \ref{fig:GUI} and \ref{fig:GUIcloseup}.

 \begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/aligned_subcloud.png} % requires the graphicx package
   \caption{After ICP is performed, the user can inspect the result to determine whether the alignment algorithm converged accurately. The right image shows the top view, to highlight the alignment}
   \label{fig:GUI_poseicp}
\end{figure}

The link will only be written to file if ICP has already been performed. The link data is recorded in human-readable CSV file format, so loop closure information from multiple sessions with the GUI can be spliced together later. Bad links can also be removed from the file by simply deleting a line of the file. 

\subsection{Implementation details}

The loop closure GUI was designed using the open-source Qt framework. The back-end point cloud processing as well as the point cloud display window rendering was done with Point Cloud Library (PCL) \cite{PCL}. The sonar and navigation data are stored in a CSV file after minimal pre-processing that uses some simple heuristics to remove spurious returns.

\section{Toward loop closure automation}

The amount and type of information that can be extracted from terrain varies widely by the sensing modality. For multibeam sonar, only range and intensity (effectively reflectivity) are returned by the sensor. This dissertation only considers information that can be obtained from the range information, though future work may investigate the value of intensity for loop closure detection purposes.

\subsection{Applying 2D image features to 3D terrain}

A wealth of research has been conducted in extracting robust, identiable features from 2D imagery. The following section details work that leverages this well-established technology for the problem of recognizing 3D terrain. It was hypothesized that since many image processing techniques treat images as if they were terrain maps, the techniques developed for imagery may be useful when dealing with actual maps of terrain. Although this work is still in its early stages, the hypothesis appears to be supported by data from an underwater canyon mapping mission.

\subsubsection{Creating images from terrain}

Many standard algorithms for extracting robust image feature descriptors only consider image intensity, discarding any color information. The resulting grayscale images can be thought of as digital elevation maps (DEMs), with pixel intensity being the ``height" of the map above that location in the image. This representation is shown in Figure \ref{fig:imAsSurf}. 

%\begin{figure}[htbp]
%   \centering
%   \includegraphics[width=\textwidth]{../graphics/imageAsSurf.png} % requires the graphicx package
%   \caption{Algorithms like SIFT and SURF treat images as digital elevation maps, with intensity plotted as height. Note how brighter areas in the photo correspond to higher peaks. The algorithms use gradient information to detect and describe local stable features.}
%   \label{fig:imAsSurf}
%\end{figure}

In order to produce an image from range data, a few steps must be taken. First, a reference plane and corresponding projection direction must be chosen. A grid is constructed on the reference plane, and the distance between a sounding an its projection onto that plane will be encoded as the ``intensity" of the resulting image. For instance, when creating a normal DEM map of the seafloor, the reference plane is horizontal $(x,y)$, and the projection direction is vertical $(z)$. However, operating around an iceberg, with vertical or even undercut sides, creates an ambiguity as to which direction the projection should occur. 

The method presented here calculates the average normal for the entire submap and uses that as the projection direction. The reference plane is set perpendicular to this such that all soundings lie on the positive-$z$ side of the plane.

When applied to the sonar submap shown in Figure \ref{fig:submap_rangeImage}, this yields the image shown in Figure \ref{fig:holyImage}. Sonar occlusions result in a number of holes or gaps in the data. This does not occur in camera imagery, as the data is collected  all at once, and projected directly onto the camera's imaging sensor. Additionally, sometimes a number of soundings project to the same pixel. To handle the latter case, the average pixel height is used in the final image. 

To fill in the gaps, a repetitive procedure of eroding the image and restoring the known pixels to their original values is used. The pixels lacking data are initialized with a value of $0$. The image is then convolved with a Gaussian kernel, which acts as a low-pass filter, effectively smearing the image, filling in some of the gaps. 

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/submap_rangeImage.png} % requires the graphicx package
   \caption{A submap of sonar measurements collected in Soquel Canyon.}
   \label{fig:submap_rangeImage}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=.8\textwidth]{../graphics/holyImage} % requires the graphicx package
   \caption{A DEM created from the data shown in Figure \ref{fig:submap_rangeImage}. Notice that since the data was recorded from multiple viewpoints and the projection is not aligned with a single camera boresight, the resulting images has a number of holes.}
   \label{fig:holyImage}
\end{figure}

The second step restores the pixels that began with valid data to their original values. The result is equivalent to bilinear interpolation in the unpopulated areas, and preserves the values of the valid data. The result of this process can be seen in Figure \ref{fig:rangeImage}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=.7\textwidth]{../graphics/lowcontrast2.png} % requires the graphicx package
   \caption{After the erode-restore process, the gaps in the image are smoothed out, but the original structure remains.}
   \label{fig:rangeImage}
\end{figure}

The last step in creating a SIFT-ready image is Contrast-Limited Adaptive Histogram Equalization (CLAHE). Algorithms like SIFT and SURF use gradient information in images to extract the features. The CLAHE process attempts to enhance details without skewing the overall contrast level of the image. \cite{clahe?}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=.7\textwidth]{../graphics/rangeImage.png} % requires the graphicx package
   \caption{Adaptive Histogram Equalization boosts the detail of the image, making it easier for algorithms like SIFT to find features.}
   \label{fig:rangeImage}
\end{figure}

Robust image features like SIFT can be extracted from the resulting image and used to look for loop closure events. Figure \ref{fig:SIFTrangeImage} shows some features extracted from such an image, and Figure \ref{fig:matches} shows how these features can be matched. Upon reimaging a portion of terrain and extracting a second set of features, matching can be performed. Techniques like RANSAC can be used to screen for false matches. If a consistent model can be found using a sufficient number of matches, there is a high likelihood that the two submaps correspond to the same terrain, and precision alignment can be performed using ICP with reduced risk of false alignment. 

\begin{figure}[htbp]
   \centering
   \includegraphics[width=.8\textwidth]{../graphics/SIFTrangeImage.png} % requires the graphicx package
   \caption{A random sampling of SIFT features extracted from the range image.}
   \label{fig:SIFTrangeImage}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/siftMatches.png} % requires the graphicx package
   \caption{SIFT feature matches between two successive passes by the Soquel Canyon wall. }
   \label{fig:matches}
\end{figure}

\subsubsection{A note on robustness}

2D image feature techniques are powerful, but they cannot create information where there is none. If the terrain is perfectly flat and featureless, they will not produce meaningful results. 

The matching techniques described above treat the features as if they lie in a plane, which is not necessarily a valid assumption. The same terrain, when projected in different directions can change appearance drastically. Using the average normal of the point cloud data as the projection direction for the range image, as described above, is a simple heuristic. However, it appears to be relatively robust to the selected plane. Figure \ref{ref:lighting} shows the images resulting from projecting the same range data in a number of directions. The central image uses the average normal plane. Above and below it represent $\pm 15$deg of elevation and left and right $\pm 15$ deg of azimuth, relative to neutral.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{../graphics/rangeImageLighting.png} % requires the graphicx package
   \caption{The range image remains relatively consistent over $\pm 15$deg of azimuth and elevation change in projection direction. Qualitatively, it looks like changes in lighting over the terrain, for which image features like SIFT and SURF claim some robustness \cite{citationneeded}.}
   \label{fig:lighting}
\end{figure}

%\section{3D Point Cloud Features}
% 
% \subsection{Using Invariant Information to Improve Feature Discriminative Power} 
%
%
% 
%Recent advances in the affordability and capability of ranging sensors has spurred research in the development of robust point cloud features akin to those used in 2D image processing. They seek to encode local information contained in the point cloud in order to classify interest points or compare with databases of known shapes. 
% 
% \subsection{Curvature and Normal Vector Estimation}
%
%Surface curvature and normal vector direction are two parameters that are often estimated from local patches of point cloud data, and can provide useful information. 
%
%A number of methods exist, but the most efficient method uses principal component analysis to fit a plane to a small neighborhood of points. 
%
%\begin{align} % requires amsmath; align* for no eq. number
%   C = ~&\frac{1}{K} \sum\limits_{k=0}^{K} (\mathbf{p}_k - \mathbf{\bar{p}})  (\mathbf{p}_k - \mathbf{\bar{p}})^\intercal \\
%   \sigma_i\mathbf{\lambda}_i = &~ C\mathbf{\lambda}_i
%\end{align}
%
%Here $\mathbf{p}_k$ are the $K$ nearest neighbors to the point of interest. $\mathbf{\bar{p}}$ can either be the mean of the patch or the interest point itself. These will be very similar except on the boundaries of a point cloud.
%
%The normal to this plane is the eigenvector corresponding to the minimum eigenvalue \cite{?}. The curvature can be estimated as the ratio of eigenvectors
%
%\begin{align} % requires amsmath; align* for no eq. number
%   \kappa = ~&\frac{\sigma_{1}}{\sigma_{1}+\sigma_{2}+\sigma_{3}} 
%\end{align}
%
%or a more complex model, such as a quadratic paraboloid can be used. The former method is simpler, but can fail in noisy point clouds where there is not a single clearly-defined surface. The latter method is used in the 3D extension of Harris Corners \cite{Harris3D}.
%
%Curvature can be used to extract features in the terrain, for later use in comparison. This solves the first problem for extracting landmarks from the terrain: ensuring that these points of interest are well-localized, as discussed in section \ref{sec.2Dfeatures}.  A simple feature descriptor can be constructed based off the curvature and normal vector to aid in the comarison. 
% 